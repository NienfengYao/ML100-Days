{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Cupoy 機器學習百日馬拉松-期末考](https://www.kaggle.com/c/ml100marathon-final-exam/)\n",
    "* Version\n",
    "  * V1: 使用 MobileNet 做 transfer learning\n",
    "  * V2: 使用 EarlyStopping/ReduceLROnPlateau，在 training 時依 val_loss 的情況來調整 learning rate.      \n",
    "  * V3: 調整最後的 FC layer，由二層調整為三層 (神經元個數也調大了)\n",
    "  * V4: 調整ImageDataGenerator 的參數，做了 rotate 與平移的設定\n",
    "  * V5: 改用 VGG16 做 transfer learning\n",
    "  * V6: 改回原 MobileNet 做 transfer learning，但將 dataset 只分成 train/valid，用更多的 train set 來做訓練。\n",
    "    * 調整 ImageDataGenerator 參數\n",
    "    * 調整 epoch 30 => 50\n",
    "    \n",
    "* Score  \n",
    "\n",
    "| Version | Private Score |Public Score |  \n",
    "|------|------|------|\n",
    "|   V1  | 0.82600 | 0.83900 |\n",
    "|   V2  | 0.91600 | 0.91900 |\n",
    "|   V3  | 0.92200 | 0.92100 |\n",
    "|   V4  | 0.92500 | 0.94500 |\n",
    "|   V5  | 0.91700 | 0.90100 |\n",
    "|   V6  | 0.93100 | 0.93700 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Version: V6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_base = '/home/ryanyao/docker_mount/data/ml100marathon-final-exam'\n",
    "train_path = os.path.join(img_base, 'train')\n",
    "test_path = os.path.join(img_base, 'test')\n",
    "sub_train_path = os.path.join(img_base, 'sub_train')\n",
    "sub_valid_path = os.path.join(img_base, 'sub_valid')\n",
    "sub_test_path = os.path.join(img_base, 'sub_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dandelion', 'tulip', 'rose', 'sunflower', 'daisy']\n"
     ]
    }
   ],
   "source": [
    "class_set = os.listdir(train_path)\n",
    "print(class_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/dandelion/ images: 687\n",
      "train/tulip/ images: 633\n",
      "train/rose/ images: 515\n",
      "train/sunflower/ images: 488\n",
      "train/daisy/ images: 500\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks\n",
    "for i in class_set:\n",
    "    print('train/%s/ images:' %(i), len(os.listdir(os.path.join(train_path, i))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy 50張圖片到 sub_valid, 其餘的 copy 到 sub_train\n",
    "def generate_dir():\n",
    "    for i_step in (sub_train_path, sub_valid_path):\n",
    "        os.mkdir(i_step)\n",
    "        \n",
    "    for i_step in (sub_train_path, sub_valid_path):\n",
    "        for i_class in class_set:\n",
    "            os.mkdir(os.path.join(i_step, i_class))\n",
    "            fnames = os.listdir(os.path.join(train_path, i_class))\n",
    "            \n",
    "            # sub_train dir\n",
    "            if (i_step == sub_train_path):\n",
    "                sub_fnames = fnames[:-100]\n",
    "            elif (i_step == sub_valid_path):\n",
    "                sub_fnames = fnames[-100:-50]\n",
    "            else:\n",
    "                sub_fnames = fnames[-50:]\n",
    "            for fname in sub_fnames:\n",
    "                src = os.path.join(train_path, i_class, fname)\n",
    "                det = os.path.join(i_step, i_class, fname)\n",
    "                # print(src)\n",
    "                # print(det)\n",
    "                shutil.copyfile(src, det)\n",
    "\n",
    "generate_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers.core import Dense, Flatten\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import load_model\n",
    "from keras.applications.mobilenet import MobileNet, preprocess_input, decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workaround Issue: \"Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\"\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2323 images belonging to 5 classes.\n",
      "Found 250 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "sub_train_batches = datagen.flow_from_directory(sub_train_path, target_size=(224,224), batch_size=batch_size)\n",
    "sub_valid_batches = datagen.flow_from_directory(sub_valid_path, target_size=(224,224), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# 檢視 image input size\n",
    "print(sub_train_batches.image_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下載 Keras NobileNet model 進行 transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports the mobilenet model and discards the last 1000 neuron layer.\n",
    "base_model = MobileNet(include_top=False, weights='imagenet') \n",
    "\n",
    "x = base_model.output\n",
    "# we add dense layers so that the model can learn more complex functions and classify for better results.\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x) \n",
    "x = Dense(1024, activation='relu')(x)\n",
    "preds = Dense(5, activation='softmax')(x) # final layer with softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=base_model.input, outputs=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check the model architecture\n",
    "# for i,layer in enumerate(model.layers):\n",
    "#     print(i, layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)    (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, None, None, 32)    864       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, None, None, 32)    128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (ReLU)            (None, None, None, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, None, None, 32)    288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, None, None, 32)    128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (ReLU)        (None, None, None, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, None, None, 64)    2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, None, None, 64)    256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (ReLU)        (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv_pad_2 (ZeroPadding2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, None, None, 64)    576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, None, None, 64)    256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (ReLU)        (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, None, None, 128)   8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, None, None, 128)   512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (ReLU)        (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, None, None, 128)   1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, None, None, 128)   512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (ReLU)        (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, None, None, 128)   16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, None, None, 128)   512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (ReLU)        (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "conv_pad_4 (ZeroPadding2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, None, None, 128)   1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, None, None, 128)   512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (ReLU)        (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, None, None, 256)   32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, None, None, 256)   1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (ReLU)        (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, None, None, 256)   2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, None, None, 256)   1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (ReLU)        (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, None, None, 256)   65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, None, None, 256)   1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (ReLU)        (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "conv_pad_6 (ZeroPadding2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, None, None, 256)   2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, None, None, 256)   1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (ReLU)        (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, None, None, 512)   131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, None, None, 512)   2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (ReLU)        (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, None, None, 512)   4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, None, None, 512)   2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (ReLU)        (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, None, None, 512)   262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, None, None, 512)   2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (ReLU)        (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, None, None, 512)   4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, None, None, 512)   2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (ReLU)        (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, None, None, 512)   262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, None, None, 512)   2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (ReLU)        (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, None, None, 512)   4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, None, None, 512)   2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (ReLU)        (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, None, None, 512)   262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, None, None, 512)   2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (ReLU)        (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, None, None, 512)   4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, None, None, 512)   2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (ReLU)       (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, None, None, 512)   262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, None, None, 512)   2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (ReLU)       (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, None, None, 512)   4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, None, None, 512)   2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (ReLU)       (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, None, None, 512)   262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, None, None, 512)   2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (ReLU)       (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "conv_pad_12 (ZeroPadding2D)  (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, None, None, 512)   4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, None, None, 512)   2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (ReLU)       (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, None, None, 1024)  524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, None, None, 1024)  4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (ReLU)       (None, None, None, 1024)  0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, None, None, 1024)  9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, None, None, 1024)  4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (ReLU)       (None, None, None, 1024)  0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, None, None, 1024)  1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, None, None, 1024)  4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (ReLU)       (None, None, None, 1024)  0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 5125      \n",
      "=================================================================\n",
      "Total params: 5,333,189\n",
      "Trainable params: 5,311,301\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 輸出整個網路結構\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 EarlyStopping/ReduceLROnPlateau，在 training 時依 val_loss 的情況來調整 learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "earlystop = EarlyStopping(monitor=\"val_loss\", patience=10, verbose=1)\n",
    "\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "reduce_lr = ReduceLROnPlateau(factor=0.3,min_lr=1e-12, monitor='val_loss', patience=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ryanyao/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n",
      " - 52s - loss: 0.7270 - acc: 0.7582 - val_loss: 0.8452 - val_acc: 0.7812\n",
      "Epoch 2/50\n",
      " - 26s - loss: 0.4965 - acc: 0.8335 - val_loss: 0.8477 - val_acc: 0.7844\n",
      "Epoch 3/50\n",
      " - 24s - loss: 0.3737 - acc: 0.8670 - val_loss: 1.3465 - val_acc: 0.6927\n",
      "Epoch 4/50\n",
      " - 22s - loss: 0.4427 - acc: 0.8622 - val_loss: 0.9361 - val_acc: 0.7156\n",
      "Epoch 5/50\n",
      " - 22s - loss: 0.3495 - acc: 0.8854 - val_loss: 0.6023 - val_acc: 0.7890\n",
      "Epoch 6/50\n",
      " - 22s - loss: 0.3312 - acc: 0.8887 - val_loss: 0.5923 - val_acc: 0.8303\n",
      "Epoch 7/50\n",
      " - 22s - loss: 0.2630 - acc: 0.9149 - val_loss: 0.8699 - val_acc: 0.7936\n",
      "Epoch 8/50\n",
      " - 23s - loss: 0.2680 - acc: 0.9109 - val_loss: 0.3712 - val_acc: 0.8945\n",
      "Epoch 9/50\n",
      " - 22s - loss: 0.2547 - acc: 0.9207 - val_loss: 0.3601 - val_acc: 0.9107\n",
      "Epoch 10/50\n",
      " - 22s - loss: 0.2648 - acc: 0.9093 - val_loss: 0.4480 - val_acc: 0.8624\n",
      "Epoch 11/50\n",
      " - 22s - loss: 0.2603 - acc: 0.9145 - val_loss: 0.8199 - val_acc: 0.8119\n",
      "Epoch 12/50\n",
      " - 22s - loss: 0.2465 - acc: 0.9198 - val_loss: 0.6231 - val_acc: 0.8440\n",
      "Epoch 13/50\n",
      " - 22s - loss: 0.2046 - acc: 0.9316 - val_loss: 0.5659 - val_acc: 0.8761\n",
      "Epoch 14/50\n",
      " - 22s - loss: 0.1898 - acc: 0.9431 - val_loss: 0.6262 - val_acc: 0.8349\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "Epoch 15/50\n",
      " - 21s - loss: 0.1083 - acc: 0.9644 - val_loss: 0.3539 - val_acc: 0.9266\n",
      "Epoch 16/50\n",
      " - 22s - loss: 0.0679 - acc: 0.9793 - val_loss: 0.3081 - val_acc: 0.9083\n",
      "Epoch 17/50\n",
      " - 23s - loss: 0.0612 - acc: 0.9796 - val_loss: 0.1593 - val_acc: 0.9554\n",
      "Epoch 18/50\n",
      " - 26s - loss: 0.0499 - acc: 0.9813 - val_loss: 0.4261 - val_acc: 0.8899\n",
      "Epoch 19/50\n",
      " - 22s - loss: 0.0460 - acc: 0.9841 - val_loss: 0.2758 - val_acc: 0.9312\n",
      "Epoch 20/50\n",
      " - 22s - loss: 0.0433 - acc: 0.9831 - val_loss: 0.3303 - val_acc: 0.9174\n",
      "Epoch 21/50\n",
      " - 22s - loss: 0.0347 - acc: 0.9900 - val_loss: 0.2713 - val_acc: 0.9266\n",
      "Epoch 22/50\n",
      " - 23s - loss: 0.0397 - acc: 0.9865 - val_loss: 0.2476 - val_acc: 0.9174\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
      "Epoch 23/50\n",
      " - 25s - loss: 0.0328 - acc: 0.9900 - val_loss: 0.3115 - val_acc: 0.9220\n",
      "Epoch 24/50\n",
      " - 22s - loss: 0.0190 - acc: 0.9931 - val_loss: 0.2497 - val_acc: 0.9174\n",
      "Epoch 25/50\n",
      " - 22s - loss: 0.0260 - acc: 0.9926 - val_loss: 0.2556 - val_acc: 0.9420\n",
      "Epoch 26/50\n",
      " - 22s - loss: 0.0158 - acc: 0.9948 - val_loss: 0.1352 - val_acc: 0.9633\n",
      "Epoch 27/50\n",
      " - 22s - loss: 0.0172 - acc: 0.9948 - val_loss: 0.3155 - val_acc: 0.9174\n",
      "Epoch 28/50\n",
      " - 22s - loss: 0.0314 - acc: 0.9909 - val_loss: 0.2234 - val_acc: 0.9404\n",
      "Epoch 29/50\n",
      " - 23s - loss: 0.0187 - acc: 0.9918 - val_loss: 0.2583 - val_acc: 0.9404\n",
      "Epoch 30/50\n",
      " - 22s - loss: 0.0143 - acc: 0.9948 - val_loss: 0.2853 - val_acc: 0.9174\n",
      "Epoch 31/50\n",
      " - 22s - loss: 0.0131 - acc: 0.9952 - val_loss: 0.2423 - val_acc: 0.9266\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
      "Epoch 32/50\n",
      " - 22s - loss: 0.0161 - acc: 0.9941 - val_loss: 0.2880 - val_acc: 0.9266\n",
      "Epoch 33/50\n",
      " - 22s - loss: 0.0129 - acc: 0.9957 - val_loss: 0.3570 - val_acc: 0.9375\n",
      "Epoch 34/50\n",
      " - 23s - loss: 0.0158 - acc: 0.9949 - val_loss: 0.3782 - val_acc: 0.9312\n",
      "Epoch 35/50\n",
      " - 23s - loss: 0.0131 - acc: 0.9958 - val_loss: 0.2568 - val_acc: 0.9450\n",
      "Epoch 36/50\n",
      " - 23s - loss: 0.0084 - acc: 0.9970 - val_loss: 0.4765 - val_acc: 0.8991\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 8.100000013655517e-06.\n",
      "Epoch 00036: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# Adam optimizer\n",
    "# loss function will be categorical cross entropy\n",
    "# evaluation metric will be accuracy\n",
    "\n",
    "step_size_sub_train = sub_train_batches.n // sub_train_batches.batch_size\n",
    "step_size_sub_valid = sub_valid_batches.n // sub_valid_batches.batch_size\n",
    "history = model.fit_generator(generator=sub_train_batches, \n",
    "                    steps_per_epoch=step_size_sub_train, \n",
    "                    validation_data=sub_valid_batches,\n",
    "                    validation_steps=step_size_sub_valid,\n",
    "                    epochs=50,\n",
    "                    callbacks=[earlystop,reduce_lr],\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 上傳 Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'daisy': 0, 'dandelion': 1, 'rose': 2, 'sunflower': 3, 'tulip': 4}\n"
     ]
    }
   ],
   "source": [
    "print(sub_train_batches.class_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.DataFrame(columns=['id', 'flower_class'])\n",
    "fnames = os.listdir(test_path)\n",
    "\n",
    "for idx, fname in enumerate(fnames):\n",
    "    # Loads an image into PIL format.\n",
    "    img = image.load_img(os.path.join(test_path, fname), target_size=(224, 224))\n",
    "    \n",
    "    # Converts a PIL Image instance to a Numpy array. shape=(224, 224, 3)\n",
    "    img_data = image.img_to_array(img)   \n",
    "    \n",
    "    # Insert a new axis that will appear at the `axis` position in the expanded array shape. shape=(1, 224, 224, 3)\n",
    "    img_data = np.expand_dims(img_data, axis=0)                \n",
    "\n",
    "    preds = model.predict(img_data)\n",
    "    submit.loc[idx] = [fname.split('.')[0], preds.argmax()]\n",
    "\n",
    "submit.to_csv('Day101_submission_v6.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
