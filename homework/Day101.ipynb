{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Cupoy 機器學習百日馬拉松-期末考](https://www.kaggle.com/c/ml100marathon-final-exam/)\n",
    "* Version\n",
    "  * V1: 使用 MobileNet 做 transfer learning\n",
    "  * V2: 使用 EarlyStopping/ReduceLROnPlateau，在 training 時依 val_loss 的情況來調整 learning rate.      \n",
    "  * V3: 調整最後的 FC layer，由二層調整為三層 (神經元個數也調大了)\n",
    "  * V4: 調整ImageDataGenerator 的參數，做了 rotate 與平移的設定\n",
    "  * V5: 改用 VGG16 做 transfer learning\n",
    "* Score  \n",
    "\n",
    "| Version | Private Score |Public Score |  \n",
    "|------|------|------|\n",
    "|   V1  | 0.82600 | 0.83900 |\n",
    "|   V2  | 0.91600 | 0.91900 |\n",
    "|   V3  | 0.92200 | 0.92100 |\n",
    "|   V4  | 0.92500 | 0.94500 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version: V5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_base = '/home/ryanyao/docker_mount/data/ml100marathon-final-exam'\n",
    "train_path = os.path.join(img_base, 'train')\n",
    "test_path = os.path.join(img_base, 'test')\n",
    "sub_train_path = os.path.join(img_base, 'sub_train')\n",
    "sub_valid_path = os.path.join(img_base, 'sub_valid')\n",
    "sub_test_path = os.path.join(img_base, 'sub_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dandelion', 'tulip', 'rose', 'sunflower', 'daisy']\n"
     ]
    }
   ],
   "source": [
    "class_set = os.listdir(train_path)\n",
    "print(class_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/dandelion/ images: 687\n",
      "train/tulip/ images: 633\n",
      "train/rose/ images: 515\n",
      "train/sunflower/ images: 488\n",
      "train/daisy/ images: 500\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks\n",
    "for i in class_set:\n",
    "    print('train/%s/ images:' %(i), len(os.listdir(os.path.join(train_path, i))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy 各50張圖片到 sub_valid/sub_test, 其餘的 copy 到 sub_train\n",
    "def generate_dir():\n",
    "    for i_step in (sub_train_path, sub_valid_path, sub_test_path):\n",
    "        os.mkdir(i_step)\n",
    "        \n",
    "    for i_step in (sub_train_path, sub_valid_path, sub_test_path):\n",
    "        for i_class in class_set:\n",
    "            os.mkdir(os.path.join(i_step, i_class))\n",
    "            fnames = os.listdir(os.path.join(train_path, i_class))\n",
    "            \n",
    "            # sub_train dir\n",
    "            if (i_step == sub_train_path):\n",
    "                sub_fnames = fnames[:-100]\n",
    "            elif (i_step == sub_valid_path):\n",
    "                sub_fnames = fnames[-100:-50]\n",
    "            else:\n",
    "                sub_fnames = fnames[-50:]\n",
    "            for fname in sub_fnames:\n",
    "                src = os.path.join(train_path, i_class, fname)\n",
    "                det = os.path.join(i_step, i_class, fname)\n",
    "                # print(src)\n",
    "                # print(det)\n",
    "                shutil.copyfile(src, det)\n",
    "\n",
    "# generate_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import load_model\n",
    "# from keras.applications.mobilenet import MobileNet, preprocess_input, decode_predictions\n",
    "from keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workaround Issue: \"Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\"\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2323 images belonging to 5 classes.\n",
      "Found 250 images belonging to 5 classes.\n",
      "Found 250 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "sub_train_batches = datagen.flow_from_directory(sub_train_path, target_size=(256,256), batch_size=batch_size)\n",
    "sub_valid_batches = datagen.flow_from_directory(sub_valid_path, target_size=(256,256), batch_size=batch_size)\n",
    "sub_test_batches = datagen.flow_from_directory(sub_test_path, target_size=(256,256), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "# 檢視 image input size\n",
    "print(sub_train_batches.image_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下載 Keras NobileNet model 進行 transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 256, 256, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#imports the mobilenet model and discards the last 1000 neuron layer.\n",
    "# base_model = MobileNet(include_top=False, weights='imagenet') \n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape = (256, 256, 3))\n",
    "base_model.summary()\n",
    "\n",
    "# Freeze the layers which you don't want to train. Here I am freezing the first 5 layers.\n",
    "for layer in base_model.layers[:5]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ryanyao/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "x = base_model.output\n",
    "# we add dense layers so that the model can learn more complex functions and classify for better results.\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation='relu')(x) \n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "preds = Dense(5, activation='softmax')(x) # final layer with softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=base_model.input, outputs=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check the model architecture\n",
    "# for i,layer in enumerate(model.layers):\n",
    "#     print(i, layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 256, 256, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              33555456  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 5125      \n",
      "=================================================================\n",
      "Total params: 49,324,869\n",
      "Trainable params: 49,212,293\n",
      "Non-trainable params: 112,576\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 輸出整個網路結構\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 EarlyStopping/ReduceLROnPlateau，在 training 時依 val_loss 的情況來調整 learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "earlystop = EarlyStopping(monitor=\"val_loss\", patience=10, verbose=1)\n",
    "\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "reduce_lr = ReduceLROnPlateau(factor=0.3,min_lr=1e-12, monitor='val_loss', patience=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ryanyao/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n",
      " - 57s - loss: 1.6113 - acc: 0.2795 - val_loss: 1.4749 - val_acc: 0.3705\n",
      "Epoch 2/50\n",
      " - 51s - loss: 1.3522 - acc: 0.4442 - val_loss: 1.1991 - val_acc: 0.4541\n",
      "Epoch 3/50\n",
      " - 45s - loss: 1.1115 - acc: 0.5513 - val_loss: 0.9564 - val_acc: 0.5780\n",
      "Epoch 4/50\n",
      " - 46s - loss: 0.8843 - acc: 0.6496 - val_loss: 0.6725 - val_acc: 0.7615\n",
      "Epoch 5/50\n",
      " - 48s - loss: 0.7814 - acc: 0.6995 - val_loss: 0.6791 - val_acc: 0.7018\n",
      "Epoch 6/50\n",
      " - 46s - loss: 0.6535 - acc: 0.7566 - val_loss: 0.5421 - val_acc: 0.8073\n",
      "Epoch 7/50\n",
      " - 47s - loss: 0.5859 - acc: 0.7811 - val_loss: 0.4320 - val_acc: 0.8394\n",
      "Epoch 8/50\n",
      " - 46s - loss: 0.5326 - acc: 0.8042 - val_loss: 0.4640 - val_acc: 0.8165\n",
      "Epoch 9/50\n",
      " - 47s - loss: 0.5166 - acc: 0.8091 - val_loss: 0.4613 - val_acc: 0.8438\n",
      "Epoch 10/50\n",
      " - 39s - loss: 0.5028 - acc: 0.8089 - val_loss: 0.3692 - val_acc: 0.8670\n",
      "Epoch 11/50\n",
      " - 41s - loss: 0.4736 - acc: 0.8321 - val_loss: 0.3622 - val_acc: 0.8716\n",
      "Epoch 12/50\n",
      " - 36s - loss: 0.4423 - acc: 0.8348 - val_loss: 0.5296 - val_acc: 0.7890\n",
      "Epoch 13/50\n",
      " - 36s - loss: 0.4125 - acc: 0.8523 - val_loss: 0.3852 - val_acc: 0.8716\n",
      "Epoch 14/50\n",
      " - 36s - loss: 0.3928 - acc: 0.8491 - val_loss: 0.3893 - val_acc: 0.8486\n",
      "Epoch 15/50\n",
      " - 35s - loss: 0.3910 - acc: 0.8527 - val_loss: 0.3414 - val_acc: 0.8716\n",
      "Epoch 16/50\n",
      " - 35s - loss: 0.3765 - acc: 0.8634 - val_loss: 0.3539 - val_acc: 0.8853\n",
      "Epoch 17/50\n",
      " - 35s - loss: 0.3757 - acc: 0.8628 - val_loss: 0.3992 - val_acc: 0.8527\n",
      "Epoch 18/50\n",
      " - 36s - loss: 0.3417 - acc: 0.8708 - val_loss: 0.3015 - val_acc: 0.8899\n",
      "Epoch 19/50\n",
      " - 35s - loss: 0.3339 - acc: 0.8763 - val_loss: 0.4775 - val_acc: 0.8440\n",
      "Epoch 20/50\n",
      " - 36s - loss: 0.3524 - acc: 0.8701 - val_loss: 0.3441 - val_acc: 0.8807\n",
      "Epoch 21/50\n",
      " - 36s - loss: 0.3230 - acc: 0.8793 - val_loss: 0.3157 - val_acc: 0.8853\n",
      "Epoch 22/50\n",
      " - 40s - loss: 0.3008 - acc: 0.8853 - val_loss: 0.3000 - val_acc: 0.9037\n",
      "Epoch 23/50\n",
      " - 37s - loss: 0.2677 - acc: 0.8991 - val_loss: 0.2893 - val_acc: 0.8991\n",
      "Epoch 24/50\n",
      " - 35s - loss: 0.2627 - acc: 0.8999 - val_loss: 0.3443 - val_acc: 0.8853\n",
      "Epoch 25/50\n",
      " - 35s - loss: 0.2585 - acc: 0.9029 - val_loss: 0.3071 - val_acc: 0.8884\n",
      "Epoch 26/50\n",
      " - 36s - loss: 0.2626 - acc: 0.9015 - val_loss: 0.3344 - val_acc: 0.9083\n",
      "Epoch 27/50\n",
      " - 35s - loss: 0.2415 - acc: 0.9120 - val_loss: 0.3449 - val_acc: 0.8624\n",
      "Epoch 28/50\n",
      " - 36s - loss: 0.2411 - acc: 0.9068 - val_loss: 0.2664 - val_acc: 0.9174\n",
      "Epoch 29/50\n",
      " - 35s - loss: 0.2229 - acc: 0.9198 - val_loss: 0.3318 - val_acc: 0.9037\n",
      "Epoch 30/50\n",
      " - 36s - loss: 0.2235 - acc: 0.9164 - val_loss: 0.3167 - val_acc: 0.8853\n",
      "Epoch 31/50\n",
      " - 37s - loss: 0.2045 - acc: 0.9304 - val_loss: 0.4029 - val_acc: 0.8945\n",
      "Epoch 32/50\n",
      " - 37s - loss: 0.2241 - acc: 0.9219 - val_loss: 0.3069 - val_acc: 0.8899\n",
      "Epoch 33/50\n",
      " - 45s - loss: 0.2017 - acc: 0.9290 - val_loss: 0.2522 - val_acc: 0.9286\n",
      "Epoch 34/50\n",
      " - 43s - loss: 0.1737 - acc: 0.9324 - val_loss: 0.3646 - val_acc: 0.8853\n",
      "Epoch 35/50\n",
      " - 42s - loss: 0.1999 - acc: 0.9256 - val_loss: 0.3515 - val_acc: 0.8945\n",
      "Epoch 36/50\n",
      " - 41s - loss: 0.1888 - acc: 0.9288 - val_loss: 0.2888 - val_acc: 0.9083\n",
      "Epoch 37/50\n",
      " - 47s - loss: 0.1766 - acc: 0.9399 - val_loss: 0.1934 - val_acc: 0.9266\n",
      "Epoch 38/50\n",
      " - 45s - loss: 0.1784 - acc: 0.9384 - val_loss: 0.3288 - val_acc: 0.8991\n",
      "Epoch 39/50\n",
      " - 45s - loss: 0.1713 - acc: 0.9386 - val_loss: 0.3334 - val_acc: 0.9083\n",
      "Epoch 40/50\n",
      " - 42s - loss: 0.1891 - acc: 0.9363 - val_loss: 0.2859 - val_acc: 0.8945\n",
      "Epoch 41/50\n",
      " - 40s - loss: 0.1545 - acc: 0.9463 - val_loss: 0.4641 - val_acc: 0.8750\n",
      "Epoch 42/50\n",
      " - 45s - loss: 0.1705 - acc: 0.9417 - val_loss: 0.3405 - val_acc: 0.9174\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 2.9999999242136255e-05.\n",
      "Epoch 43/50\n",
      " - 43s - loss: 0.1323 - acc: 0.9570 - val_loss: 0.2573 - val_acc: 0.9174\n",
      "Epoch 44/50\n",
      " - 43s - loss: 0.1279 - acc: 0.9544 - val_loss: 0.3281 - val_acc: 0.9037\n",
      "Epoch 45/50\n",
      " - 42s - loss: 0.1302 - acc: 0.9573 - val_loss: 0.3261 - val_acc: 0.9128\n",
      "Epoch 46/50\n",
      " - 44s - loss: 0.1231 - acc: 0.9549 - val_loss: 0.3116 - val_acc: 0.9128\n",
      "Epoch 47/50\n",
      " - 44s - loss: 0.1179 - acc: 0.9609 - val_loss: 0.3105 - val_acc: 0.9174\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 8.999999772640877e-06.\n",
      "Epoch 00047: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# Adam optimizer\n",
    "# loss function will be categorical cross entropy\n",
    "# evaluation metric will be accuracy\n",
    "\n",
    "step_size_sub_train = sub_train_batches.n // sub_train_batches.batch_size\n",
    "step_size_sub_valid = sub_valid_batches.n // sub_valid_batches.batch_size\n",
    "history = model.fit_generator(generator=sub_train_batches, \n",
    "                    steps_per_epoch=step_size_sub_train, \n",
    "                    validation_data=sub_valid_batches,\n",
    "                    validation_steps=step_size_sub_valid,\n",
    "                    epochs=50,\n",
    "                    callbacks=[earlystop,reduce_lr],\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5149751326867512, 0.8660714285714286]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_size_sub_test = sub_test_batches.n // sub_test_batches.batch_size\n",
    "model.evaluate_generator(generator=sub_test_batches, steps=step_size_sub_test, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 上傳 Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'daisy': 0, 'dandelion': 1, 'rose': 2, 'sunflower': 3, 'tulip': 4}\n"
     ]
    }
   ],
   "source": [
    "print(sub_train_batches.class_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.DataFrame(columns=['id', 'flower_class'])\n",
    "fnames = os.listdir(test_path)\n",
    "\n",
    "for idx, fname in enumerate(fnames):\n",
    "    # Loads an image into PIL format.\n",
    "    img = image.load_img(os.path.join(test_path, fname), target_size=(256, 256))\n",
    "    \n",
    "    # Converts a PIL Image instance to a Numpy array. shape=(224, 224, 3)\n",
    "    img_data = image.img_to_array(img) \n",
    "    img_data = img_data/255.0\n",
    "    \n",
    "    # Insert a new axis that will appear at the `axis` position in the expanded array shape. shape=(1, 224, 224, 3)\n",
    "    img_data = np.expand_dims(img_data, axis=0)                \n",
    "\n",
    "    preds = model.predict(img_data)\n",
    "    submit.loc[idx] = [fname.split('.')[0], preds.argmax()]\n",
    "\n",
    "submit.to_csv('Day101_submission_v5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
